{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' !pip install beautifulsoup4\\n!pip install numpy \\n!pip install pandas\\n!pip install requests '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "!python -m pip install beautifulsoup4\n",
    "!python -m pip install numpy \n",
    "!python -m pip install pandas\n",
    "!python -m pip install requests \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "input_csv_file = \"KnowledgeURL.csv\"  # Path to CSV with URLs\n",
    "output_txt_file = \"../data/combined_web_content.txt\"  # Output text file\n",
    "input_file = output_txt_file  # For cleaning step, use the previous output file\n",
    "cleaned_output_file = \"../data/cleaned_data/cleaned_combined_web_content.txt\"  # Cleaned output text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract main content from a webpage, prioritizing the <main> tag\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # First try to extract content from the <main> tag\n",
    "        main = soup.find('main')\n",
    "        if main:\n",
    "            text_content = main.get_text(separator=\"\\n\")\n",
    "            return text_content\n",
    "        \n",
    "        # If no <main> tag, try to find <article> tag (common for blogs)\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            text_content = article.get_text(separator=\"\\n\")\n",
    "            return text_content\n",
    "        \n",
    "        # Try to find a common main content div by class or id\n",
    "        main_content = soup.find('div', {'class': 'main-content'}) or soup.find('div', {'id': 'main-content'})\n",
    "        if main_content:\n",
    "            text_content = main_content.get_text(separator=\"\\n\")\n",
    "            return text_content\n",
    "\n",
    "        # Fallback: Extract text from all <p> tags if no specific containers found\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text_content = '\\n'.join([p.get_text() for p in paragraphs])\n",
    "        \n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch or parse {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read URLs from CSV and extract text content\n",
    "def extract_content_from_urls(csv_file):\n",
    "    df = pd.read_csv(csv_file, header=None)  # Read CSV without headers initially\n",
    "    if df.shape[1] == 1:  # If there's only one column, assume it's the URL\n",
    "        df.columns = ['URL']  # Add header dynamically\n",
    "    urls = df['URL']\n",
    "    \n",
    "    all_text_content = \"\"\n",
    "    for url in urls:\n",
    "        print(f\"Extracting content from: {url}\")\n",
    "        text_content = extract_text_from_url(url)\n",
    "        all_text_content += text_content + \"\\n\\n\"\n",
    "    \n",
    "    return all_text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from: URL\n",
      "Failed to fetch or parse URL: Invalid URL 'URL': No scheme supplied. Perhaps you meant https://URL?\n",
      "Extracting content from: https://en.wikipedia.org/wiki/Housing_in_the_United_Kingdom\n",
      "Extracting content from: https://en.wikipedia.org/wiki/Affordability_of_housing_in_the_United_Kingdom\n",
      "Content extracted and saved to ../data/combined_web_content.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the combined text into a .txt file\n",
    "def save_text_to_file(text, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "# Extract content from URLs and save to output file\n",
    "combined_text = extract_content_from_urls(input_csv_file)\n",
    "save_text_to_file(combined_text, output_txt_file)\n",
    "\n",
    "print(f\"Content extracted and saved to {output_txt_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned content written to ../data/cleaned_data/cleaned_combined_web_content.txt\n"
     ]
    }
   ],
   "source": [
    "# Clean empty lines from a file and save the cleaned version\n",
    "def clean_empty_lines(input_file, output_file):\n",
    "    # Open the input file and read its lines\n",
    "    with open(input_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter out empty lines (strip removes any surrounding whitespace)\n",
    "    cleaned_lines = [line for line in lines if line.strip()]\n",
    "\n",
    "    # Write the cleaned lines to the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "\n",
    "    print(f\"Cleaned content written to {output_file}\")\n",
    "\n",
    "# Clean the combined text file\n",
    "clean_empty_lines(input_file, cleaned_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 74737 characters from the file.\n",
      "First 500 characters:\n",
      "Toggle the table of contents\n",
      "Housing in the United Kingdom\n",
      "3 languages\n",
      "Deutsch\n",
      "日本語\n",
      "Simple English\n",
      "Edit links\n",
      "Article\n",
      "Talk\n",
      "English\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "Tools\n",
      "Tools\n",
      "move to sidebar\n",
      "hide\n",
      "Actions\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "General\n",
      "What links here\n",
      "Related changes\n",
      "Upload file\n",
      "Special pages\n",
      "Permanent link\n",
      "Page information\n",
      "Cite this page\n",
      "Get shortened URL\n",
      "Download QR code\n",
      "Print/export\n",
      "Download as PDF\n",
      "Printable version\n",
      "In other projects\n",
      "Wikimedia Commons\n",
      "Wikidata item\n",
      "Appearance\n",
      "move to sidebar\n",
      "hide\n",
      "From W\n"
     ]
    }
   ],
   "source": [
    "# Fetch article content from a local file\n",
    "def fetch_article_content(file_path):\n",
    "    try:\n",
    "        # Open and read the file content\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Process the content similar to how you processed HTML content\n",
    "        # Split into lines, remove extra spaces\n",
    "        lines = (line.strip() for line in content.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read or process the file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Fetch and process the cleaned content\n",
    "file_content = fetch_article_content(cleaned_output_file)\n",
    "print(f\"Fetched {len(file_content)} characters from the file.\")\n",
    "print(\"First 500 characters:\")\n",
    "print(file_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the DataFrame: 3158\n"
     ]
    }
   ],
   "source": [
    "# Split the file content into data chunks and create a DataFrame\n",
    "data = []\n",
    "lines = file_content.split(\"\\n\")\n",
    "for line in lines:\n",
    "    if line:  # Check if the line is not empty\n",
    "        data.append({\"content\": line})\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head(3)\n",
    "\n",
    "# Print the total number of rows in the DataFrame\n",
    "print(f\"Total number of rows in the DataFrame: {len(df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
