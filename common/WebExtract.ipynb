{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.25.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" !pip install beautifulsoup4\n",
    "!pip install numpy \n",
    "!pip install pandas\n",
    "!pip install requests \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "input_csv_file = \"KnowledgeURL.csv\"  # Path to CSV with URLs\n",
    "output_txt_file = \"../data/cleaned_data/combined_web_content.txt\"  # Output text file\n",
    "input_file = output_txt_file  # For cleaning step, use the previous output file\n",
    "cleaned_output_file = \"../data/cleaned_data/cleaned_combined_web_content.txt\"  # Cleaned output text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract main content from a webpage, prioritizing the <main> tag\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # First try to extract content from the <main> tag\n",
    "        main = soup.find('main')\n",
    "        if main:\n",
    "            text_content = main.get_text(separator=\"\\n\")\n",
    "            return text_content\n",
    "        \n",
    "        # If no <main> tag, try to find <article> tag (common for blogs)\n",
    "        article = soup.find('article')\n",
    "        if article:\n",
    "            text_content = article.get_text(separator=\"\\n\")\n",
    "            return text_content\n",
    "        \n",
    "        # Try to find a common main content div by class or id\n",
    "        main_content = soup.find('div', {'class': 'main-content'}) or soup.find('div', {'id': 'main-content'})\n",
    "        if main_content:\n",
    "            text_content = main_content.get_text(separator=\"\\n\")\n",
    "            return text_content\n",
    "\n",
    "        # Fallback: Extract text from all <p> tags if no specific containers found\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text_content = '\\n'.join([p.get_text() for p in paragraphs])\n",
    "        \n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch or parse {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read URLs from CSV and extract text content\n",
    "def extract_content_from_urls(csv_file):\n",
    "    df = pd.read_csv(csv_file, header=None)  # Read CSV without headers initially\n",
    "    if df.shape[1] == 1:  # If there's only one column, assume it's the URL\n",
    "        df.columns = ['URL']  # Add header dynamically\n",
    "    urls = df['URL']\n",
    "    \n",
    "    all_text_content = \"\"\n",
    "    for url in urls:\n",
    "        print(f\"Extracting content from: {url}\")\n",
    "        text_content = extract_text_from_url(url)\n",
    "        all_text_content += text_content + \"\\n\\n\"\n",
    "    \n",
    "    return all_text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from: URL\n",
      "Failed to fetch or parse URL: Invalid URL 'URL': No scheme supplied. Perhaps you meant https://URL?\n",
      "Extracting content from: https://www.savills.co.uk/blog/article/363940/residential-property/top-tips-for-landlords-and-international-students-in-london-s-rental-market.aspx\n",
      "Extracting content from: https://www.timeout.com/london/property/how-to-navigate-the-london-student-housing-market\n",
      "Extracting content from: https://www.zoopla.co.uk/discover/renting/complete-guide-to-student-renting-in-london/\n",
      "Failed to fetch or parse https://www.zoopla.co.uk/discover/renting/complete-guide-to-student-renting-in-london/: 403 Client Error: Forbidden for url: https://www.zoopla.co.uk/discover/renting/complete-guide-to-student-renting-in-london/\n",
      "Extracting content from: https://www.londonnest.com/blog/tips-and-trick-to-find-your-student-accommodation-in-london/\n",
      "Extracting content from: https://rib.co.uk/management-professional/property-advice/the-students-guide-to-finding-accommodation/\n",
      "Extracting content from: https://www.thehomelike.com/blog/how-to-find-student-accommodation-in-london/\n",
      "Extracting content from: https://stuhomes.com/blog/five-tips-for-finding-student-accommodation-in-london\n",
      "Extracting content from: https://amberstudent.com/blog/post/student-housing-guide-london\n",
      "Extracting content from: https://www.wearefreemovers.com/student-accommodation-tips-to-find-housing-in-and-around-london/\n",
      "Extracting content from: https://en.wikipedia.org/wiki/Housing_in_the_United_Kingdom\n",
      "Extracting content from: https://www.tripadvisor.com/Attractions-g186338-Activities-c57-t70-London_England.html\n",
      "Failed to fetch or parse https://www.tripadvisor.com/Attractions-g186338-Activities-c57-t70-London_England.html: 403 Client Error: Forbidden for url: https://www.tripadvisor.com/Attractions-g186338-Activities-c57-t70-London_England.html\n",
      "Extracting content from: https://www.timeout.com/london/things-to-do/londons-major-parks\n",
      "Extracting content from: https://www.ellisandco.co.uk/guides/area-guides/green-areas-in-london/\n",
      "Failed to fetch or parse https://www.ellisandco.co.uk/guides/area-guides/green-areas-in-london/: 403 Client Error: Forbidden for url: https://www.ellisandco.co.uk/guides/area-guides/green-areas-in-london/\n",
      "Extracting content from: https://sanctum.london/safest-places-to-stay-in-london/\n",
      "Extracting content from: https://en.uhomes.com/blog/safest-areas-in-london\n",
      "Failed to fetch or parse https://en.uhomes.com/blog/safest-areas-in-london: 403 Client Error: Forbidden for url: https://en.uhomes.com/blog/safest-areas-in-london\n",
      "Extracting content from: https://www.crownluxuryhomes.com/safest-neighbourhoods-in-london/\n",
      "Content extracted and saved to data/combined_web_content.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the combined text into a .txt file\n",
    "def save_text_to_file(text, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "# Extract content from URLs and save to output file\n",
    "combined_text = extract_content_from_urls(input_csv_file)\n",
    "save_text_to_file(combined_text, output_txt_file)\n",
    "\n",
    "print(f\"Content extracted and saved to {output_txt_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned content written to data/cleaned_combined_web_content.txt\n"
     ]
    }
   ],
   "source": [
    "# Clean empty lines from a file and save the cleaned version\n",
    "def clean_empty_lines(input_file, output_file):\n",
    "    # Open the input file and read its lines\n",
    "    with open(input_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter out empty lines (strip removes any surrounding whitespace)\n",
    "    cleaned_lines = [line for line in lines if line.strip()]\n",
    "\n",
    "    # Write the cleaned lines to the output file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "\n",
    "    print(f\"Cleaned content written to {output_file}\")\n",
    "\n",
    "# Clean the combined text file\n",
    "clean_empty_lines(input_file, cleaned_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 146548 characters from the file.\n",
      "First 500 characters:\n",
      "The Savills Blog\n",
      "Top tips for landlords and international students in London’s rental market\n",
      "09 July 2024\n",
      "Blog Article\n",
      "Contacts & Related Articles\n",
      "At the start of summer, international students in their thousands begin searching for a property to rent in London. Many have never lived away from home before and navigating the process of finding somewhere isn’t always easy.\n",
      "Meanwhile, landlords who’ve previously let to corporate or family tenants might see themselves letting their property to a stu\n"
     ]
    }
   ],
   "source": [
    "# Fetch article content from a local file\n",
    "def fetch_article_content(file_path):\n",
    "    try:\n",
    "        # Open and read the file content\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        # Process the content similar to how you processed HTML content\n",
    "        # Split into lines, remove extra spaces\n",
    "        lines = (line.strip() for line in content.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read or process the file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Fetch and process the cleaned content\n",
    "file_content = fetch_article_content(cleaned_output_file)\n",
    "print(f\"Fetched {len(file_content)} characters from the file.\")\n",
    "print(\"First 500 characters:\")\n",
    "print(file_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the DataFrame: 2893\n"
     ]
    }
   ],
   "source": [
    "# Split the file content into data chunks and create a DataFrame\n",
    "data = []\n",
    "lines = file_content.split(\"\\n\")\n",
    "for line in lines:\n",
    "    if line:  # Check if the line is not empty\n",
    "        data.append({\"content\": line})\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head(3)\n",
    "\n",
    "# Print the total number of rows in the DataFrame\n",
    "print(f\"Total number of rows in the DataFrame: {len(df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "202410ClaudeAPICall_kernel",
   "language": "python",
   "name": "202410claudeapicall_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
